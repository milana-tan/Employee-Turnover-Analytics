# -*- coding: utf-8 -*-
"""Milana_Block_33_CareerSim_StarterCode_Updated (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18mp-DDbbOdx_Z_GYH1eRLt9mKdEUSVf4

# Block 33: Career Simulation
## Employee Turnover Analytics

### Project Statement:

Portobello Tech is an app innovator that has devised an intelligent way of predicting employee turnover within the company. It periodically evaluates employees' work details, including the number of projects they worked on, average monthly working hours, time spent in the company, promotions in the last 5 years, and salary level.

Data from prior evaluations show the employees’ satisfaction in the workplace. The data could be used to identify patterns in workstyle and their interest in continuing to work for the company.  

The HR department owns the data and uses it to predict employee turnover. Employee turnover refers to the total number of workers who leave a company over a certain period of time.

As the ML developer assigned to the HR department, you have been asked to create ML programs to

* Perform a data quality check by checking for missing values.

* Understand what factors contributed most to employee turnover by EDA.

* Perform a clustering of employees who left based on their satisfaction and evaluation.

* Handle the left-class imbalance using the SMOTE technique.

* Perform k-fold cross-validation model training and evaluate performance.  

* Identify the best model and justify the evaluation metrics used.  

* Suggest various retention strategies for targeted employees.

### Data will be modified from:  

https://www.kaggle.com/liujiaqi/hr-comma-sepcsv
![Block_33_PS_datatable.png](attachment:Block_33_PS_datatable.png)

### Perform the following steps:

* Perform a data quality check by checking for missing values if any.

* Understand what factors contributed most to employee turnover by EDA.

  * Draw a heatmap of the correlation matrix between all numerical features/columns in the data.

  * Draw the distribution plot of  

    * Employee satisfaction (use column satisfaction_level)

    * Employee evaluation (use column last_evaluation)

    * Employee average monthly hours (use column average_montly_hours)

  * Draw the bar plot of the employee project count of both employees who left and who stayed in the organization (use column number_project and hue column turnover) and give your inferences from the plot.

* Perform a clustering of employees who left based on their satisfaction and evaluation.

  * Choose columns satisfaction_level, last_evaluation, and turnover.

  * K-means clustering of employees who left the company into 3 clusters.

  * Based on the satisfaction and evaluation factors, give your thoughts on the employee clusters.

* Handle the left-class imbalance using the SMOTE technique.

  * Pre-process the data by converting categorical columns to numerical columns by:

    * Separating categorical variables and numeric variables

    * Applying get_dummies() to the categorical variables

    * Combining categorical variables and numeric variables



  * Do the stratified split of the dataset to train and test in the ratio 80:20 with random_state=123.

  * Upsample the training dataset using the SMOTE technique from the `imblearn` module.

* Perform 5-fold cross-validation model training and evaluate performance.  

  * Train a logistic regression model, apply a 5-fold CV, and plot the classification report.

  * Train a random forest classifier model, apply the 5-fold CV, and plot the classification report.

  * Train a gradient boosting classifier model, apply the 5-fold CV, and plot the classification report.

* Identify the best model and justify the evaluation metrics used.  

  * Find the ROC/AUC for each model and plot the ROC curve.

  * Find the confusion matrix for each of the models.

  * Explain which metric needs to be used: Recall or Precision?

* Suggest various retention strategies for targeted employees.

  * Using the best model, predict the probability of employee turnover in the test data.

  * Based on the probability score range below, categorize the employees into four zones and suggest your thoughts on the retention strategies for each zone.

* Safe Zone (Green) (Score <= 20%)

* Low Risk Zone (Yellow) (20% < Score <= 60%)

* Medium Risk Zone (Orange) (60% < Score < 90%)

* High Risk Zone (Red) (Score => 90%).

### We'll be covering:
* Descriptive Analytics: What happened?
* Predictive Analytics: What Might Happen?
* Prescriptive Analytics: What Should We Do?


***
### Objective:
To understand what factors contributed most to employee turnover.

To perform clustering of employees who left based on their satisfaction and evaluation

To create a model that predicts the likelihood that a certain employee will leave the company or not.

To create or improve different retention strategies for targeted employees.

<a id='dataset'></a>
#### Reading the Data
***
"""

# Commented out IPython magic to ensure Python compatibility.
# Import the neccessary modules for data manipulation and visual representation
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as matplot
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")
# %matplotlib inline

# Load the dataset
# TODO: Complete this section
df = pd.read_csv("HR_comma_sep.csv")

# Examine the dataset
# TODO: Complete this section
df.shape
df.info()
df.isnull().sum()
df.describe()

# Rename Columns
df = df.rename(columns={'satisfaction_level': 'satisfaction',
                        'last_evaluation': 'evaluation',
                        'number_project': 'projectCount',
                        'average_montly_hours': 'averageMonthlyHours',
                        'time_spend_company': 'yearsAtCompany',
                        'Work_accident': 'workAccident',
                        'promotion_last_5years': 'promotion',
                        'sales' : 'department',
                        'left' : 'turnover'
                        })

# Print the first few rows
df.head()

# Check class (turnover) distribution
# TODO: Complete this section
left_counts = df['turnover'].value_counts()
print("Class Counts:\n", left_counts)

# Plot barplot of class distribution
# TODO: Complete this section
sns.countplot(x='turnover', data=df)
plt.title('Turnover Class Distribution')
plt.xlabel('Turnover (0 = Stayed, 1 = Left)')
plt.ylabel('Count')
plt.show()

"""<a id='datacleaning'></a>
##### 1. Perform data quality check by checking for missing values if any
***
"""

# Can you check to see if there are any missing values in our data set?
# TODO: Complete this section
missing_values = df.isnull().sum()
print("Missing Values in Each Column:\n", missing_values)

# Check the datatype of our features. Are there any data inconsistencies?
# TODO: Complete this section

for col in df:
    print(f"{col} unique values: {df[col].unique()}")
duplicates = df[df.duplicated()]
print(f"Number of duplicate rows: {duplicates.shape[0]}")
print(df.describe())
print(df.info())

"""<a id='descriptive_statistics'></a>
##### 2. Understand what factors contributed most to employee turnover by EDA
***
"""

# Display the statistical overview of the employees
# TODO: Complete this section

stats = df.describe()
print(stats)

# Display the mean summary of Employees (Turnover V.S. Non-turnover). What do you notice between the groups?

# Group data by turnover, return turnover dataframe.
def compute_turnover_Summary():
    # TODO: Implement this function
    turnover_summary = df.groupby('turnover')
    return turnover_summary

# TODO: Call function compute_turnover_Summary()
# Store turnover dataframe in variable called turnover_Summary
turnover_Summary = compute_turnover_Summary()
print(turnover_Summary)

# Calculate mean of Turnover V.S. Non-turnover employees.
round(turnover_Summary.mean(), 2)

# Calculate std of Turnover V.S. Non-turnover employees
round(turnover_Summary.std(), 2)

"""<a id='correlation_matrix'></a>
### Correlation Matrix
"""

# Create a correlation matrix. What features correlate the most with turnover? What other correlations did you find?
# TODO: Complete this section

correlation_matrix = df.corr()

# Draw a heatmap of the correlation matrix between all numerical features/columns in the data.
# Add title, xticklabels and yticklabels
# TODO: Complete this section
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

"""<a id='eda1'></a>
###### Distribution of Satisfaction, Evaluation, and Monthly Hours

**Task:** Draw the distribution plot of:
- Employee satisfaction
- Employee evaluation
- Employee average monthly hours
"""

# TODO: Complete this section
# Plot the distribution of Employee Satisfaction, Evaluation, and average monthly hours. What story can you tell?

# TODO: Complete this section
# Set up the matplotlib figure
plt.figure(figsize=(12, 8))


# TODO: Complete this section
# Graph Employee Satisfaction
sns.distplot(kde=False)

plt.subplot(3, 1, 1)
sns.histplot(df['satisfaction'], kde=True)
plt.title('Employee Satisfaction Distribution')
plt.xlabel('Satisfaction Level')
plt.ylabel('Frequency')


# TODO: Complete this section
# Graph Employee Evaluation
sns.distplot()

plt.subplot(3, 1, 2)
sns.histplot(df['evaluation'], kde=True)
plt.title('Employee Evaluation Distribution')
plt.xlabel('Evaluation Score')
plt.ylabel('Frequency')


# TODO: Complete this section
# Graph Employee Average Monthly Hours
sns.distplot()

plt.subplot(3, 1, 3)
sns.histplot(df['averageMonthlyHours'], kde=True)
plt.title('Employee Average Monthly Hours Distribution')
plt.xlabel('Average Monthly Hours')
plt.ylabel('Frequency')

plt.tight_layout()

"""<a id='project_count'></a>
##### Bar plot of employee project count

**Task:**
- Draw the bar plot of the employee project count of both employees who left and who stayed in the organization (use column number_project and hue column turnover)
- Give your inferences from the plot
"""

# TODO: Complete this section
# Set up the matplotlib figure

plt.figure(figsize=(10, 6))

# TODO: Complete this section
# Create bar plot of project count of both, employees who left and who stayed

sns.countplot(data=df, x='projectCount', hue='turnover', palette='Set2')

# TODO: Complete this section
# Set xlabel and ylabel

plt.xlabel('Number of Projects')
plt.ylabel('Count of Employees')
plt.title('Employee Project Count: Left vs Stayed')

plt.show()

"""- More than half of the employees with **2, 6 and 7** projects left the company
- Majority of the employees who did not leave the company had **3,4, and 5** projects
- All of the employees with **7** projects left the company
- There is an increase in employee turnover rate as project count increases

<a id='clustering'></a>
##### 3. Perform clustering of Employees who left based on their satisfaction and evaluation
***

**Task:**
- Perform a clustering of employees who left based on their satisfaction and evaluation.
- Choose columns satisfaction_level, last_evaluation, and turnover.
- K-means clustering of employees who left the company into 3 clusters.
- Based on the satisfaction and evaluation factors, give your thoughts on the employee clusters.
"""

# Import KMeans Model
from sklearn.cluster import KMeans

# Graph and create 3 clusters of Employee Turnover
kmeans = KMeans(n_clusters=3,random_state=2)
kmeans.fit(df[df.turnover==1][["satisfaction","evaluation"]])

kmeans_colors = ['green' if c == 0 else 'blue' if c == 2 else 'red' for c in kmeans.labels_]

fig = plt.figure(figsize=(10, 6))
plt.scatter(x="satisfaction",y="evaluation", data=df[df.turnover==1],
            alpha=0.25,color = kmeans_colors)
plt.xlabel("Satisfaction")
plt.ylabel("Evaluation")
plt.scatter(x=kmeans.cluster_centers_[:,0],y=kmeans.cluster_centers_[:,1],color="black",marker="X",s=100)
plt.title("Clusters of Employee Turnover")

plt.show();

"""**Cluster 1 (Blue):** Hard-working and Sad Employees

**Cluster 2 (Red):** Bad and Sad Employee

**Cluster 3 (Green):** Hard-working and Happy Employee

`There are 3 distinct clusters for employees who left the company`

**Cluster 1 (Hard-working and Sad Employee):** Satisfaction was below 0.2 and evaluations were greater than 0.75. Which could be a good indication that employees who left the company were good workers but felt horrible at their job.
 - **Question:** What could be the reason for feeling so horrible when you are highly evaluated? Could it be working too hard? Could this cluster mean employees who are "overworked"?

**Cluster 2 (Bad and Sad Employee):** Satisfaction between about 0.35~0.45 and evaluations below ~0.58. This could be seen as employees who were badly evaluated and felt bad at work.
 - **Question:** Could this cluster mean employees who "under-performed"?

**Cluster 3 (Hard-working and Happy Employee):** Satisfaction between 0.7~1.0 and evaluations were greater than 0.8. Which could mean that employees in this cluster were "ideal". They loved their work and were evaluated highly for their performance.
 - **Question:** Could this cluser mean that employees left because they found another job opportunity?

__Observations__
- Cluster 1:
Yes that could be a possibility due to bad working conditions, feeling underappreciated, or taken advantage of.


- Cluster 2:
It could be a lack of motivation or not getting validation they need to feel appreciated.


- Cluster 3:
They could be happier and feel capable with their responsibilities of work or appreciated the most.

<a id='pre_processing'></a>
#### Pre-processing
***

##### 4.Handle the ***left*** Class Imbalance using SMOTE technique.

##### 4a. Pre-Process the data by converting categorical columns to numerical
- Separate categorical variables and numeric variables.
- Apply **get_dummies()** to the categorical variables.
- Combine categorical variables and numeric variables.
"""

# TODO: Implement this function
from imblearn.over_sampling import SMOTE

def compute_new_df():
    # TODO: Implement this function

    # Separate categorical variables and numeric variables into separate dataframes.
        categorical_columns = df.select_dtypes(include=['object']).columns
        numeric_columns = df.select_dtypes(exclude=['object']).columns

    # Apply get_dummies() to the categorical variables.
        categorical_df = pd.get_dummies(df[categorical_columns], drop_first=False)

    # Combine categorical variables and numeric variables into one dataframe and return single combined dataframe.
        numeric_df = df[numeric_columns]
        new_df = pd.concat([numeric_df, categorical_df], axis=1)

        return new_df
    # Name combined dataframe new_df
new_df = compute_new_df()

new_df.shape

# TODO: Complete this section
# Print first few rows of new_df
new_df.head(5)

"""##### 4b. Split Train/Test Set

##### Let's split our data into a train and test set. We'll fit our model with the train set and leave our test set for our last evaluation.

- Do the stratified split of the dataset to train and test in the ratio 80:20 with random_state=123.
"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, confusion_matrix, precision_recall_curve

# TODO: Complete this section
# Create the X and y set
y = new_df['turnover']
X = new_df.drop(columns=['turnover'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=123)

print(X_train.shape)
print(X_test.shape)

"""##### 4c. Class Imbalance

##### Employee Turnover Rate: 24%
"""

round(df.turnover.value_counts(1), 2)

"""##### 4c. Upsample the train dataset using SMOTE technique"""

from sklearn.utils import resample
from imblearn.over_sampling import SMOTE

# Upsample using SMOTE
sm = SMOTE(random_state=12, sampling_strategy=1.0)
x_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)

print("Original shape:", X_train.shape, y_train.shape)
print ("SMOTE sample shape:", x_train_sm.shape, y_train_sm.shape)

from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score

"""##### Perform 5-Fold cross validation model training and evaluate performance
***

- Train a logistic regression model, apply a 5-fold CV, and plot the classification report.
- Train a random forest classifier model, apply the 5-fold CV, and plot the classification report.
- Train a gradient boosting classifier model, apply the 5-fold CV, and plot the classification report.

#### We're training 3 models:

1. Logistic Regression Classifier
2. Random Forest Classifier
3. Gradient Boosting Classifier

<a id='lr'></a>
## Logistic Regression Classifier
"""

from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score

lr = LogisticRegression()
lr = lr.fit(x_train_sm, y_train_sm)
lr

"""### Apply 5-Fold Cross Validation on Logistic Regression"""

# TODO: Complete this section
# Apply 5-Fold Cross Validation on Logistic Regression

logreg = LogisticRegression(random_state=123, max_iter=1000, solver='lbfgs')

cv_scores = cross_val_score(logreg, x_train_sm, y_train_sm, cv=5, scoring='accuracy')

"""### Logistic Regression Classifier AUC  (0.77)"""

print ("\n\n ---Logistic Regression Model---")
lr_auc = roc_auc_score(y_test, lr.predict(X_test))

print ("Logistic Regression AUC = %2.2f" % lr_auc)

print(classification_report(y_test, lr.predict(X_test)))

"""<a id='rf'></a>
# Random Forest Classifier
"""

from sklearn.ensemble import RandomForestClassifier

# Random Forest Model
rf = RandomForestClassifier()
rf = rf.fit(x_train_sm, y_train_sm)
rf

"""### Apply 5-Fold Cross Validation on Random Forest"""

# TODO: Complete this section
# Apply 5-Fold Cross Validation on Random Forest

rf = RandomForestClassifier(random_state=123, n_estimators=100)
rf.fit(x_train_sm, y_train_sm)
cv_scores = cross_val_score(rf, x_train_sm, y_train_sm, cv=5, scoring='accuracy')

"""### Random Forest Classifier AUC (0.98)"""

from sklearn.metrics import roc_auc_score

print ("\n\n ---Random Forest Model---")
rf_roc_auc = roc_auc_score(y_test, rf.predict(X_test))
print ("Random Forest AUC = %2.2f" % rf_roc_auc)
print(classification_report(y_test, rf.predict(X_test)))

"""<a id='svc'></a>
# Gradient Boosting Classifier

"""

from sklearn.ensemble import GradientBoostingClassifier

gbc = GradientBoostingClassifier()
gbc = gbc.fit(x_train_sm,y_train_sm)
gbc

"""### Apply 5-Fold Cross Validation on Gradient Boosting Classifier"""

# TODO: Complete this section
# Apply 5-Fold Cross Validation on Gradient Boosting Classifier

gbc = GradientBoostingClassifier(random_state=123)
gbc = gbc.fit(x_train_sm,y_train_sm)

cv_scores = cross_val_score(gbc, x_train_sm, y_train_sm, cv=5, scoring='accuracy')

"""### Gradient Boosting Classifier AUC  (0.96)"""

from sklearn.metrics import roc_auc_score

print ("\n\n ---Gradient Boosting Model---")
gbc_auc = roc_auc_score(y_test, gbc.predict(X_test))
print ("Gradient Boosting Classifier AUC = %2.2f" % gbc_auc)
print(classification_report(y_test, gbc.predict(X_test)))

"""# Model Evaluation (F1 Score & ROC/AUC)

<a id='roc'></a>
# ROC Graph
"""

# Create ROC Graph
from sklearn.metrics import roc_curve
from sklearn.metrics import confusion_matrix

fpr, tpr, thresholds = roc_curve(y_test, lr.predict_proba(X_test)[:,1])
rf_fpr, rf_tpr, rf_thresholds = roc_curve(y_test, rf.predict_proba(X_test)[:,1])
gbc_fpr, gbc_tpr, gbc_thresholds = roc_curve(y_test, gbc.predict_proba(X_test)[:,1])


plt.figure(figsize=(15,12))

# Plot Logistic Regression ROC
plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % lr_auc)

# Plot Random Forest ROC
plt.plot(rf_fpr, rf_tpr, label='Random Forest Classifier (area = %0.2f)' % rf_roc_auc)

# Plot Decision Tree ROC
plt.plot(gbc_fpr, gbc_tpr, label='Gradient Boosting Classifier (area = %0.2f)' % gbc_auc)

# Plot Base Rate ROC
plt.plot([0,1], [0,1],label='Base Rate')

plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Graph')
plt.legend(loc="lower right")
plt.show();

# Confusion Matrix for Logistic Regresion
# TODO: Complete this section

y_pred_lr = lr.predict(X_test)
cm = confusion_matrix(y_test, y_pred_lr)

print("Confusion Matrix:")
print(cm)

# Confusion Matrix for GBC
# TODO: Complete this section

y_pred_gbc = gbc.predict(X_test)
cm_gbc = confusion_matrix(y_test, y_pred_gbc)

print("Confusion Matrix for Gradient Boosting Classifier:")
print(cm_gbc)

# Confusion Matrix for Random Forest
# TODO: Complete this section

y_pred_rf = rf.predict(X_test)
cm_rf = confusion_matrix(y_test, y_pred_rf)

print("Confusion Matrix for Random Forest Classifier:")
print(cm_rf)

"""# Recall or Precision?
- What do precision and recall tell us? Which one should we optimize? Why?

Observations:
Whenever trying to reduce false positives, or making sure that people will actually leave when the model predicts they will, precision is crucial.
When minimizing false negatives, getting the most at-risk employees as possible, regardless of whether this means keeping those who aren't truly at risk—recall is important.

##### 7. Suggest various retention strategies on targeted employees.

### 7a. Using the best model, predict the probability of employee turnover in the test data

### Retention Plan
"""

# Ranking turnover probability for employees
# TODO: Predict the probability of employee turnover in the test data.

turnover_probabilities = rf.predict_proba(X_test)[:, 1]

# Turnover (0 or 1)
# Prediction probabilities are in this order

rf.classes_

"""**Task:**
- Based on the probability score range below, categorize the employees into four zones and suggest your thoughts on the retention strategies for each zone.
- Safe Zone (Green) (Score <= 20%)
- Low Risk Zone (Yellow) (20% < Score <= 60%)
- Medium Risk Zone (Orange) (60% < Score < 90%)
- High Risk Zone (Red) (Score => 90%).
"""

# Probability of employee turnover in subset of test data.
list(rf.predict_proba(X_test)[175:185, 1])

# TODO: Complete this section

# Categorize the employees into Safe Zone

safe_zone = probabilities <= 0.20
print(f"Employees in Safe Zone: {safe_zone.sum()}")

# TODO: Complete this section

# Categorize the employees into Low Risk Zone

low_risk_zone = (probabilities > 0.20) & (probabilities <= 0.60)
print(f"Employees in the Low Risk Zone: {low_risk_zone.sum()}")

# TODO: Complete this section

# Categorize the employees into Medium Risk Zone

med_risk_zone = (probabilities > 0.60) & (probabilities < 0.90)
print(f"Employees in the Medium Risk Zone: {med_risk_zone.sum()}")

# TODO: Complete this section

# Categorize the employees into High Risk Zone

high_risk_zone = probabilities <= 0.90
print(f"Employees in the High Risk Zone: {high_risk_zone.sum()}")

"""### Suggest your thoughts on the retention strategies for each zone. What actions are required for the different zones?

Observations:
For the safe zone employees, being consistent in interactions with rewards/recognition.
With the low risk zone employees, it is best to do check-ins and offer new challenges or training to keep them engaged and show that you care about their development.
As for the medium risk zone employees, you could help with managing their workload and request feedback where management can improve support.
Lastly, the high risk zone employees could be checked on and more management support regarding mental health or lifestyle.

## Conclusion

## What to Optimize
**Binary Classification**: Turnover V.S. Non Turnover

Instance Scoring: Likelihood of employee responding to an offer or incentive to save them from leaving

Need for application: save employees from leaving

In our employee retention problem, rather than simply predicting whether an employee will leave the company within a certain time frame, we would much rather have an estimate of the probability that he or she will leave the company. We would rank employees by their probability of leaving, then allocate a limited incentive budget to the highest-probability instances.

Solution 1:

We can rank employees by their probability of leaving, then allocate a limited incentive budget to the highest-probability instances.

OR, we can allocate our incentive budget to the instances with the highest expected loss, for which we'll need the probability of turnover.



---


Solution 2:

Develop learning programs for managers, then use analytics to gauge their performance and measure progress.

Be a good coach. Empower the team and do not micromanage.

Express interest in team members success.

Have a clear vision or strategy for the team.

Help the team with career development.

# Selection Bias
***

One thing to note about the dataset is the turnover feature. We don't know if the employees that left are interns, contractors, full-time, or part-time. These are important variables to take into consideration when applying a machine learning algorithm to them.

Evaluation is heavily subjective and can vary tremendously depending on the evaluator. If the employee knows the evaluator, then he or she will probably have a higher score.
"""